{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Title: create sentiment filter from reddit\n",
    "\n",
    "#### Objective:\n",
    "Develop a model to analyze sentiments from tweets and Reddit comments to create trading signals for cryptocurrencies. Showcase skills to future employers.\n",
    "\n",
    "#### Key Features:\n",
    "1. Data scraping from  Reddit\n",
    "2. Preprocessing of text data\n",
    "3. Implementation of sentiment analysis using pre-trained\n",
    "4. Visualization of results\n",
    "\n",
    "## Approach\n",
    "\n",
    "- get the most important subreddits for specific coin \n",
    "- get the most important general subreddits for crypto \n",
    "- get submission from subreddits by using the spcefied methods(.hot() .new() ...)\n",
    "- get comments from the submissions\n",
    "- filter comments by specific keywords that suggest indicative value (maybe improved by clustering)\n",
    "\n",
    "the problem i keep running into is waht weight (importance) has a comment and how do I value its score regarding to time:\n",
    "solution idea:\n",
    "\n",
    "1. categorize by score \n",
    "2. categorize by creation time \n",
    "3. give more value to comments that have higher score and are more recent \n",
    "4. less value to less recent and smaller score \n",
    "\n",
    "Essentially, what should happen is the comment gets a positive, neutral, or negative score. This means if the comment has a high comment score, it should weigh the amount of the score. For example, a comment with a score of 0 to 5 counts as 1 comment. If a comment has a score of 5 to 10, it counts as 2 comments, and so on. Additionally, if these comments have been made in the past, let's say 1 day ago, these comments should be worth less.\n",
    "\n",
    "```python\n",
    "# Function to calculate weighted score\n",
    "def calculate_weighted_score(row, current_date):\n",
    "    # Categorize by score\n",
    "    if row['comment_score'] <= 5:\n",
    "        score_weight = 1\n",
    "    elif row['comment_score'] <= 10:\n",
    "        score_weight = 2\n",
    "    elif row['comment_score'] <= 20:\n",
    "        score_weight = 3\n",
    "    else:\n",
    "        score_weight = 4\n",
    "    \n",
    "    # Categorize by creation time (more recent comments get higher weight)\n",
    "    days_since_creation = (current_date - row['comment_created_utc']).days\n",
    "    if days_since_creation <= 1:\n",
    "        time_weight = 1\n",
    "    elif days_since_creation <= 2:\n",
    "        time_weight = 0.8\n",
    "    elif days_since_creation <= 3:\n",
    "        time_weight = 0.6\n",
    "    else:\n",
    "        time_weight = 0.4\n",
    "    \n",
    "    # Combine weights\n",
    "    weighted_score = score_weight * time_weight\n",
    "    return weighted_score\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Phases:\n",
    "\n",
    "Idea: \n",
    "\n",
    "\n",
    "Outcome: create an indicator that you can combine with technical indicators increase accuracy for trading by gauging the current sentimenet of people. the sentiment should be updated everyday to catch shift in sentument.\n",
    "\n",
    "\n",
    "### **Phase 1: Data Collection**\n",
    "\n",
    "**Objective:**\n",
    "retrive data from PRAW\n",
    "\n",
    "Key Questions: \n",
    "how to use praw and what are the data formarts i get ? \n",
    "how to create batches to respect rate limits?  \n",
    "\n",
    "2. Set up Reddit API using PRAW.\n",
    "3. Write scripts to scrape historical data from Reddit.\n",
    "\n",
    "\n",
    "\n",
    "### Problem Statement for Phase 2: Preprocessing and Clustering\n",
    "\n",
    "**Objective:** Develop a system to preprocess and cluster subreddit data to analyze sentiment and trends related to specific coins.\n",
    "\n",
    "1. **search subreddits**\n",
    "  - search specific subreddit by name and shortname \n",
    "  - preset general subreddits that are crypto specific \n",
    "\n",
    "\n",
    "1. **Retrieve Subreddit Data:**\n",
    "   - Use the `.hot()`, `.new()`, `.controversial()`, `.rising()`, and `.top()` methods to get posts from various subreddits.\n",
    "\n",
    "2. **Combine Data:**\n",
    "   - Combine the retrieved posts into a single dataframe for further processing.\n",
    "\n",
    "3. **Filter Posts:**\n",
    "   - Filter the posts using specific keywords and phrases related to particular coins.\n",
    "   - Remove duplicate entries to ensure data integrity.\n",
    "\n",
    "4. **Retrieve Comments:**\n",
    "   - Extract all comments from the relevant posts and add them to the dataframe.\n",
    "\n",
    "5. **Filter Comments:**\n",
    "   - Filter the comments again using the same keywords and phrases to ensure relevance to the particular coin.\n",
    "\n",
    "6. **Sort Comments:**\n",
    "   - Sort the filtered comments by time to analyze trends over a specific period.\n",
    "\n",
    "7. **Sentiment Analysis:**\n",
    "   - Use CryptoBERT to assign numerical sentiment values to the submissions, enabling quantitative analysis of sentiment trends.\n",
    "\n",
    "\n",
    "Key Questions:\n",
    "\n",
    "\n",
    "What keywords and phrases should be used to filter posts and comments for specific coins?\n",
    "\n",
    "how to factor in scores on post and comments? \n",
    "\n",
    "\n",
    "What value should be given to recent data in sentiment analysis?\n",
    "Should the valuation approach resemble an Exponential Moving Average (EMA), where more weight is placed on the most recent sentiment?\n",
    "Should past data be given no value at all?\n",
    "What is an appropriate timeframe for a daily sentiment metric?\n",
    "\n",
    "\n",
    "**Phase 4: Testing and Documentation**\n",
    "1. Conduct thorough testing of the entire system to ensure accuracy and reliability.\n",
    "\n",
    "2. Document the code and project, including a detailed README file with instructions.\n",
    "3. Create visualizations of the results (e.g., sentiment trends, word clouds) using libraries like Matplotlib and Seaborn.\n",
    "4. Finalize documentation and visualizations.\n",
    "5. Prepare a presentation or report for future employers.\n",
    "\n",
    "#### Expected Outcome:\n",
    "A functional model that accurately classifies sentiments from tweets and Reddit comments, providing insights into social media trends regarding cryptocurrency trading. A well-documented project showcasing your skills to future employers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly Used Methods\n",
    "\n",
    "1. get all the subreddits with the methods .hot() .new() .controverisal() .rising() .top() \n",
    "\n",
    "2. combine in dataframe \n",
    "\n",
    "3. filter post using keywords and phrases for particular coin \n",
    "    - get rid of double entries \n",
    "\n",
    "4. get all comments from relevant posts and add to dataframe \n",
    "\n",
    "5. filter post using keywords and phrases for particular coin again\n",
    "\n",
    "6. sort comments by time \n",
    "\n",
    "7. use cryptobert to give submissions numerical value \n",
    "\n",
    "challange of valuation: what value is is given to recent data ? maybe it should go a little bit like an ema where there is more weight on the most recent sentiment or maybe now value at all to the past ? what is a good time frame daily metric ? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`subreddit.hot(limit=10)`\n",
    "- **Description:** Fetches the hot posts from the subreddit. Hot posts are those that are currently popular and receiving a lot of attention.\n",
    "1. cluster post using keywordsand phrases \n",
    "\n",
    "\n",
    "\n",
    "`subreddit.new(limit=10)`\n",
    "- **Description:** Fetches the newest posts from the subreddit, sorted from newest to oldest.\n",
    "1. cluster post using keywordsand phrases \n",
    "\n",
    "\n",
    "`subreddit.top(limit=10, time_filter='month')`\n",
    "- **Description:** Fetches the top posts from the subreddit based on score. You can specify a time filter (e.g., 'day', 'week', 'month', 'year', 'all').\n",
    "1. cluster post using keywordsand phrases \n",
    "\n",
    "\n",
    "`subreddit.controversial(limit=10)`\n",
    "- **Description:** Fetches the most controversial posts from the subreddit. Controversial posts are those with a high number of upvotes and downvotes.\n",
    "1. cluster post using keywordsand phrases \n",
    "\n",
    "\n",
    "`subreddit.rising(limit=10)`\n",
    "- **Description:** Fetches posts that are gaining popularity quickly.\n",
    "1. cluster post using keywordsand phrases \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Pitfalls and Solutions\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - **Pitfall:** API rate limits and data access restrictions.\n",
    "     - **Solution:** Use libraries like Tweepy for Twitter and PRAW for Reddit, which handle rate limits and provide robust API access.\n",
    "   - **Pitfall:** Inconsistent data formats and missing data.\n",
    "     - **Solution:** Implement data validation and cleaning scripts to handle inconsistencies and missing values.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - **Pitfall:** Handling large volumes of text data efficiently.\n",
    "     - **Solution:** Use SpaCy for efficient text preprocessing and leverage its built-in functions for tokenization, stop words removal, etc.\n",
    "   - **Pitfall:** Ensuring text data is properly cleaned and standardized.\n",
    "     - **Solution:** Use pre-written scripts for common preprocessing tasks like lowercasing, removing special characters, and stemming/lemmatization.\n",
    "\n",
    "3. **Model Development:**\n",
    "   - **Pitfall:** Training models from scratch can be time-consuming and computationally expensive.\n",
    "     - **Solution:** Use pre-trained models from the Hugging Face Transformers library (e.g., BERT, RoBERTa) and fine-tune them on your dataset.\n",
    "   - **Pitfall:** Hyperparameter tuning and model optimization.\n",
    "     - **Solution:** Use libraries like Scikit-learn for hyperparameter tuning (e.g., GridSearchCV) and model evaluation.\n",
    "\n",
    "4. **Integration:**\n",
    "   - **Pitfall:** Integrating the sentiment analysis model with the data pipeline.\n",
    "     - **Solution:** Write modular code and use functions to encapsulate different parts of the pipeline, making integration easier.\n",
    "\n",
    "5. **Visualization:**\n",
    "   - **Pitfall:** Creating meaningful and clear visualizations.\n",
    "     - **Solution:** Use libraries like Matplotlib and Seaborn for creating visualizations. Leverage pre-written scripts for common visualizations like sentiment trends and word clouds.\n",
    "\n",
    "6. **Testing and Documentation:**\n",
    "   - **Pitfall:** Ensuring thorough testing and comprehensive documentation.\n",
    "     - **Solution:** Implement unit tests using libraries like PyTest to ensure code reliability. Document the project as you go to avoid missing details.\n",
    "\n",
    "### Leveraging Pre-Written Libraries and Scripts\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - **Tweepy:** For accessing Twitter API and handling rate limits.\n",
    "   - **PRAW:** For accessing Reddit API and handling data retrieval.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - **SpaCy:** For efficient text preprocessing (tokenization, stop words removal, lemmatization).\n",
    "   - **NLTK:** For additional text processing tasks (e.g., stemming, POS tagging).\n",
    "\n",
    "3. **Model Development:**\n",
    "   - **Hugging Face Transformers:** For using and fine-tuning pre-trained models like BERT and RoBERTa.\n",
    "   - **Scikit-learn:** For model evaluation, hyperparameter tuning, and additional machine learning tasks.\n",
    "\n",
    "4. **Visualization:**\n",
    "   - **Matplotlib:** For creating basic plots and visualizations.\n",
    "   - **Seaborn:** For creating more advanced and aesthetically pleasing visualizations.\n",
    "   - **WordCloud:** For generating word cloud visualizations.\n",
    "\n",
    "### Example Workflow with Libraries\n",
    "\n",
    "1. **Data Collection:**\n",
    "   ```python\n",
    "   import tweepy\n",
    "   import praw\n",
    "\n",
    "   # Twitter API setup\n",
    "   auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "   auth.set_access_token(access_token, access_token_secret)\n",
    "   api = tweepy.API(auth)\n",
    "\n",
    "   # Reddit API setup\n",
    "   reddit = praw.Reddit(client_id='your_client_id', client_secret='your_client_secret', user_agent='your_user_agent')\n",
    "\n",
    "   # Data collection scripts\n",
    "   def collect_twitter_data(query, count):\n",
    "       tweets = api.search(q=query, count=count)\n",
    "       return [tweet.text for tweet in tweets]\n",
    "\n",
    "   def collect_reddit_data(subreddit, limit):\n",
    "       subreddit = reddit.subreddit(subreddit)\n",
    "       return [submission.title for submission in subreddit.hot(limit=limit)]\n",
    "   ```\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   ```python\n",
    "   import spacy\n",
    "\n",
    "   nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "   def preprocess_text(text):\n",
    "       doc = nlp(text)\n",
    "       tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "       return ' '.join(tokens)\n",
    "   ```\n",
    "\n",
    "3. **Model Development:**\n",
    "   ```python\n",
    "   from transformers import pipeline\n",
    "\n",
    "   # Load pre-trained sentiment analysis model\n",
    "   sentiment_analysis = pipeline('sentiment-analysis')\n",
    "\n",
    "   def analyze_sentiment(text):\n",
    "       return sentiment_analysis(text)\n",
    "   ```\n",
    "\n",
    "4. **Visualization:**\n",
    "   ```python\n",
    "   import matplotlib.pyplot as plt\n",
    "   import seaborn as sns\n",
    "\n",
    "   def plot_sentiment_distribution(sentiments):\n",
    "       sns.countplot(x=sentiments)\n",
    "       plt.title('Sentiment Distribution')\n",
    "       plt.show()\n",
    "   ```\n",
    "\n",
    "By identifying potential pitfalls and leveraging pre-written libraries and scripts, you can streamline your project and work more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
